{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling Sandbox - Lilian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, pretrained: str = \"gpt2\"):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(pretrained)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = GPT2Model.from_pretrained(pretrained)\n",
    "    \n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor,\n",
    "                past_key_values: Union[None, Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "                past_attention_mask: Union[None, torch.LongTensor] = None) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        :param input_ids: (batch_size, input_id_len) token indices\n",
    "        :param attention_mask: (batch_size, input_id_len) 1 for tokens that should be attended to, 0 for padding, etc. that shouldn't\n",
    "        :param past_key_values: Optional, num_layers-len tuple of tensors (2, batch_size, num_heads, past_tokens_len, embed_size_per_head)\n",
    "                                the cache output of a previous call to the language model that stores all the hidden states\n",
    "                                Note: tokens represented in past_key_values should NOT be included in input_ids\n",
    "        :param past_attention_mask: Optional, (batch_size, past_tokens_len) attention mask\n",
    "                                    corresponding to past key values inputs\n",
    "        :return: (last_hidden_state, new_attention_mask, key_values)\n",
    "                 last_hidden_state: (batch_size, input_id_len, hidden_size) final hidden state associated with each input_id\n",
    "                 new_attention_mask: (batch_size, past_tokens_len + input_id_len) new past attention mask\n",
    "                 key_values: updated past_key_values\n",
    "        \"\"\"\n",
    "        if past_key_values is not None:\n",
    "            new_attention_mask = torch.cat([past_attention_mask, attention_mask], dim=1)\n",
    "        else:\n",
    "            new_attention_mask = attention_mask\n",
    "        last_hidden_state, key_values = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=new_attention_mask,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        return last_hidden_state, new_attention_mask, key_values\n",
    "    \n",
    "    def tokenize(self, inputs: List[str], add_space: bool = False) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        :param inputs: list of strings to tokenize\n",
    "        :param add_space: True if a space should be added to the strings so that the first token is considered a new word,\n",
    "                          else False\n",
    "        :return: (input_ids, attention_mask)\n",
    "                 each item has shape (batch_size=len(inputs), seq_len) where seq_len is the max # of tokens in any input\n",
    "        \"\"\"\n",
    "        if add_space:\n",
    "            inputs = [\" \" + x for x in inputs]\n",
    "        \n",
    "        tokens = self.tokenizer.batch_encode_plus(\n",
    "            inputs,\n",
    "            padding=\"longest\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "    \n",
    "    def token_embedding(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the token embedding tensor of shape (vocab_size, hidden_size).\n",
    "        \"\"\"\n",
    "        return self.model.wte.weight.data\n",
    "    \n",
    "    def position_embedding(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the position embedding tensor of shape (max_seq_len, hidden_size).\n",
    "        \"\"\"\n",
    "        return self.model.wpe.weight.data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Notes\n",
    "A few things re: how GPT2Model/Tokenizer works that aren't immediately clear from the documentation:\n",
    "\n",
    "1) Let past_key_values be the tuple associated with some `past_tokens_len` words. Let the number of input ids per batch be `input_id_len`. Then the shape of the attention mask passed into the model should be `past_tokens_len + input_id_len` - in other words, we need to keep the attention of the past tokens as well, and concatenate it in front of the new attention.\n",
    "\n",
    "2) The way the GPT2Tokenizer works is that words succeeding spaces are prefixed by \"Ġ\", i.e. \"Ġword\". So, the token at the very beginning of a text sample with no preceding space, or subword parts that aren't first, do not have the Ġ. This is important to consider if we want to append a new word to a sequence, for example in the following case:\n",
    "\n",
    "We are interested in decoding the sentence \"I am happy.\", which would be represented as:\n",
    "    ['<|endoftext|>' (implicitly), 'I', 'Ġam', 'Ġhappy', '.']\n",
    "\n",
    "We tokenize and feed in '<|endoftext|>', the EOS token. Some feature is produced which can help predict \"I\".\n",
    "We tokenize and feed in 'I', using the past_key_values associated with the output of the previous step.\n",
    "We find that the next word is 'am'. We tokenize the \"am\" string to get the id associated with 'am', which is NOT EQUAL to 'Ġam'!\n",
    "\n",
    "This is just something to take note of in case we explicitly pass in strings. Ideally, the token we output from our overall predictor is directly 'Ġam' instead of 'am', which would bypass the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72007e52630541b282166961905a1a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e14ad6c56d473c9b38b82e5b857137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1357326c95d4827914b2edbf4cbeb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11d861041a742888d7b06a5af1a3a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=764.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cd4ca336da47e68fa5fb811e9ebc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3247202234.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'h.24.attn.masked_bias', 'h.25.attn.masked_bias', 'h.26.attn.masked_bias', 'h.27.attn.masked_bias', 'h.28.attn.masked_bias', 'h.29.attn.masked_bias', 'h.30.attn.masked_bias', 'h.31.attn.masked_bias', 'h.32.attn.masked_bias', 'h.33.attn.masked_bias', 'h.34.attn.masked_bias', 'h.35.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = LanguageModel(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1280])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model.wte(torch.FloatTensor([[m.tokenizer.eos_token_id]]).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
